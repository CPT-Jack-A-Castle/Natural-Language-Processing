{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "import sys\n",
    "from itertools  import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_sents(file_path, as_zip=True, to_idx=False, token_vocab=None, target_vocab=None): \n",
    "    '''\n",
    "    Function to load data from train/test files\n",
    "    '''\n",
    "    targets = []\n",
    "    inputs = []\n",
    "    zip_inps = []\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            sent, tags = line.split(\"\\t\")\n",
    "            words = [token_vocab[w.strip()] if to_idx else w.strip() for w in sent.split()]\n",
    "            ner_tags = [target_vocab[w.strip()] if to_idx else w.strip() for w in tags.split()] \n",
    "            inputs.append(words)\n",
    "            targets.append(ner_tags)\n",
    "            zip_inps.append(list(zip(words, ner_tags)))\n",
    "    return zip_inps if as_zip else (inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cw_cl_counter(corpus, min_counts = 3):\n",
    "    '''\n",
    "    Current word-current label dictionary\n",
    "    It doesn't include features with a less tham min_counts frequency, default: 3\n",
    "    '''\n",
    "    cw_cl = []\n",
    "    for s in corpus:\n",
    "        cw_cl += [word+\"_\"+label for word,label in s]\n",
    "    counter = Counter(cw_cl)\n",
    "    return {k:v for k,v in counter.items() if v >= min_counts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def phi_1(x, y, cw_cl_counts):\n",
    "    '''\n",
    "    Given a sentence x and a sequence y, returns the counts for each\n",
    "    current word-current label feature in the sentence that is also in the corpus (cw_cl_counts)\n",
    "    '''\n",
    "    features = []\n",
    "    for i in range(len(x)):\n",
    "        f = x[i]+\"_\"+y[i]\n",
    "        if f in cw_cl_counts:\n",
    "            features.append(x[i]+\"_\"+y[i])\n",
    "    return Counter(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(x,y,phi_counts):\n",
    "    '''\n",
    "    Given a sentence x, a sequence y, and a dictionary phi_counts ({phi_functions:counter_functions})\n",
    "    returns a combined dictionary of features\n",
    "    '''\n",
    "    features = {}\n",
    "    for phi_func,counts in phi_counts.items():\n",
    "        for feature, count in phi_func(x,y,counts).items():\n",
    "            features[feature] = count\n",
    "        \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perceptron(corpus,w,c,phi_counts):\n",
    "    '''\n",
    "    Given a copus, a w (dict), a number of updates c (int) and a phi_counts ({phi_functions:counter_functions})\n",
    "    performs an iteration of the perceptron\n",
    "    '''\n",
    "    for k,s in enumerate(corpus):\n",
    "        # initialize variables\n",
    "        x = [i[0] for i in s]\n",
    "        y_target = [i[1] for i in s]\n",
    "        y_pred = predict(x,w,phi_counts)\n",
    "        if y_pred != y_target:\n",
    "            # w = w + phi(x,y ) - phi(x ,y_pred)\n",
    "            phi_diff = Counter(phi(x,y_target,phi_counts))\n",
    "            phi_diff.subtract(phi(x,y_pred,phi_counts))\n",
    "            \n",
    "            for feature,count in phi_diff.items():\n",
    "                if feature in w:\n",
    "                    w[feature] += count\n",
    "                else:\n",
    "                    w[feature] = count\n",
    "        c += 1\n",
    "    return w,c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(corpus, phi_counts, num_iter = 10, averaging = True):\n",
    "    '''\n",
    "    Perform the perceptron training algorithm given a corpus, a phi_counts dictionary ({phi_functions:counter_functions}),\n",
    "    a number of iterations num_iter (int) with/without averaging (boolean)\n",
    "    '''\n",
    "    c = 0\n",
    "    w_c = {}\n",
    "    w_avg = {}\n",
    "    for iteration in range(num_iter):\n",
    "        print(\"Iteration {}/{}\".format(iteration+1,num_iter))\n",
    "        # randomize\n",
    "        shuffle(corpus)\n",
    "        w_c,c = perceptron(corpus,w_c,c,phi_counts)\n",
    "        if averaging:\n",
    "            for feature, v in w_c.items():\n",
    "                if feature in w_avg:\n",
    "                    w_avg[feature] += v\n",
    "                else:\n",
    "                    w_avg[feature] = v\n",
    "    if averaging:\n",
    "        for f,v in w_avg.items():\n",
    "            w_avg[f] = v/c\n",
    "        return w_avg\n",
    "    else:\n",
    "        return w_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x,w,phi_counts):\n",
    "    '''\n",
    "    Predicts a sequence y_pred given a sentence x, weights w (dict),\n",
    "    and phi_counts dictionary ({phi_functions:counter_functions})\n",
    "    '''\n",
    "    max_score = -1e10\n",
    "    y_pred = []\n",
    "    # all possible sequences\n",
    "    all_possible_y = product(labels, repeat = len(x))\n",
    "    for y in all_possible_y:\n",
    "        score = 0\n",
    "        # if len(y) == len(x): This should always be true\n",
    "        phi_xy = phi(x,y,phi_counts)\n",
    "        # for each feature that is in phi and w (features>0)\n",
    "        for feature in set(phi_xy).intersection(set(w)):\n",
    "            score += phi_xy[feature]*w[feature]\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            y_pred = y\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_dataset_sents(\"train.txt\")\n",
    "test_corpus = load_dataset_sents(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_cl_counts = cw_cl_counter(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['O', 'ORG', 'MISC', 'PER', 'LOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron with 5 iterations and averaging\n",
      "Phi1\n",
      "Iteration 1/5\n",
      "Iteration 2/5\n",
      "Iteration 3/5\n",
      "Iteration 4/5\n",
      "Iteration 5/5\n"
     ]
    }
   ],
   "source": [
    "print(\"Perceptron with 5 iterations and averaging\")\n",
    "# phi_1\n",
    "print(\"Phi1\")\n",
    "phi_counts = {phi_1:cw_cl_counts}\n",
    "w_phi1 = train(corpus,phi_counts, num_iter = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(w,corpus,phi_counts):\n",
    "    '''\n",
    "    Predicts a sequence for each sentence in the corpus and computes the f1score with\n",
    "    the correct and predicted flatten sequences\n",
    "    '''\n",
    "    predicted, correct = [], []\n",
    "    for s in corpus:\n",
    "        x = [i[0] for i in s]\n",
    "        correct += [i[1] for i in s]\n",
    "        predicted += predict(x,w,phi_counts)\n",
    "    \n",
    "    f1_micro = f1_score(correct, predicted, average = \"micro\",\n",
    "                        labels = ['ORG', 'MISC', 'PER', 'LOC'])\n",
    "\n",
    "    return f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "F1 Score: test 0.628\n",
      "Top 10 most positively-weighted features:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing...\")\n",
    "# print(\"F1 Score: train\",round(test(w_phi1,corpus,phi_counts),3))\n",
    "print(\"F1 Score: test\",round(test(w_phi1,test_corpus,phi_counts),3))\n",
    "print(\"Top 10 most positively-weighted features:\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
